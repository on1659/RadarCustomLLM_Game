# LLM 추론모델 프로젝트 로그

## 📌 프로젝트 요약 (새 세션용)

### 환경
- Mac mini M4 16GB, macOS
- llama.cpp: `~/llama.cpp/build/bin/llama-server`
- Python venv: `~/Work/LLM/crawler/venv/` (RAG/크롤러 공용)
- GitHub: https://github.com/on1659/RadarCustomLLM_Game

### 파인튜닝 모델
- 베이스: Qwen2.5-3B-Instruct
- 방법: Unsloth + LoRA (r=16), Colab T4 GPU
- 학습 데이터: 121개 chain-of-thought (`<think>추론</think>답변`)
- GGUF: `~/llama.cpp/models/Qwen2.5-3B-Instruct.Q4_K_M.gguf` (1.93GB)
- Colab 노트북: `~/.openclaw/workspace/reasoning-model/colab_notebook.ipynb`
- 학습 데이터 JSON: `~/.openclaw/workspace/reasoning-model/training_data.json`

### RAG 시스템
- 크롤러: `~/Work/LLM/crawler/`
  - `namu_crawler.py` — 나무위키 (오버워치 15개, 마크 12개, 팰월드 1개)
  - `palworld_crawler.py` — palworld.gg (57개 문서)
  - 크롤링 데이터: `~/Work/LLM/crawler/data/`
- RAG: `~/Work/LLM/rag/`
  - `ingest.py` — 벡터화 (FAISS + ko-sroberta-multitask)
  - `web.py` — 웹 UI (localhost:3333, 하이브리드 검색)
  - `chat.py` — CLI 챗봇
  - 벡터DB: `~/Work/LLM/rag/faiss_db/` (4,064청크)
- 임베딩: `jhgan/ko-sroberta-multitask` (한국어 특화)
- 검색: **하이브리드 (벡터 유사도 + BM25 키워드)** ← 2/16 업그레이드
- 게임명 자동 필터: 팰월드/오버워치/마크 키워드 감지

### 실행 방법
```bash
# 간편 시작 (권장) ⭐
startllm

# 상태 확인
statusllm

# 전체 중지
stopllm

# 또는 수동 실행 (기존 방법)
# 1. LLM 서버
cd ~/llama.cpp && ./build/bin/llama-server -m models/Qwen2.5-3B-Instruct.Q4_K_M.gguf -c 1024 -ngl 99 --port 8090

# 2. RAG 웹 UI
cd ~/Work/LLM/rag && source ../crawler/venv/bin/activate && python3 web.py
# → localhost:3333

# 3. 외부 접속 (ngrok)
ngrok http 3333

# 4. 크롤링 데이터 추가 후 벡터DB 재생성
cd ~/Work/LLM/rag && source ../crawler/venv/bin/activate && python3 ingest.py
```

### 현재 이슈 & TODO
- [ ] `<think>` 추론 태그 아직 안 나옴 → 학습 데이터 500개로 확장 필요
- [ ] 3B 모델 한국어 코딩 능력 약함 → 128GB PC에서 70B 모델 시도 예정
- [ ] 팰월드 나무위키 하위문서 크롤링 URL 수정 필요
- [ ] 16GB 메모리 부족으로 llama-server 가끔 kill됨 → `-c 1024`로 완화
- [ ] palworld.gg 크롤링 데이터 인코딩 깨짐 문제 → 재크롤링 필요

### Colab 사용 팁
- wandb 물어보면 **3** 입력 (시각화 스킵)
- 무료 GPU 할당량 소진 시 다른 구글 계정 사용
- 학습 완료 후 바로 Drive 저장 셀 실행 (세션 끊기면 파일 소실)
- Colab 가이드: `~/Work/LLM/colab_guide.md`

---

## 2026-02-15 22:10 — 프로젝트 시작 & Colab 노트북 생성

### 목표
- 로컬에서 돌릴 수 있는 LLM 추론모델(chain-of-thought) 만들기
- Mac mini M4 16GB 환경에서 실행

### 결정한 방법
1. **Google Colab (무료 T4 GPU)** 에서 Unsloth + LoRA로 파인튜닝
2. **GGUF Q4_K_M**으로 양자화 변환
3. **맥미니 llama.cpp**에서 Metal 가속으로 로컬 실행

### 한 일
- Colab 노트북 생성: `~/.openclaw/workspace/reasoning-model/colab_notebook.ipynb`
- 베이스 모델: **Qwen2.5-3B-Instruct** (4bit)
- LoRA 설정: r=16, target=q/k/v/o/gate/up/down_proj
- 학습 데이터 형식: `<think>추론과정</think>\n\n최종답변`
- 예시 데이터 5개 포함 (한국어: 교통, 프로그래밍, 수학, 과학, IT)
- SFTTrainer 설정: batch=2, grad_accum=4, epoch=3, lr=2e-4
- GGUF 변환 + Google Drive 저장 코드 포함

### 결과
- 노트북 완성, Colab 업로드 후 바로 실행 가능
- 아직 실행은 안 함

### 다음 할 일
- [ ] 학습 데이터 확충 (현재 5개 → 최소 500~1000개 필요)
- [ ] Colab에서 실제 학습 실행
- [ ] 학습 결과 테스트 (추론 품질 확인)
- [ ] GGUF 변환 후 맥미니에서 로컬 실행 테스트

### 참고
- 맥미니 llama.cpp 환경: `~/llama.cpp`, Metal GPU 가속 빌드 완료
- 기존 Llama 3.2 3B 모델: `~/llama.cpp/models/llama-3.2-3b-q4_k_m.gguf`
- llama-server 포트: 8090
- 16GB 제한으로 큰 모델(7B+)은 빡빡함

---

---

========================================
## 2026-02-16 00:45 — 학습 데이터 생성 & Colab 준비
========================================

### 한 일
- 학습 데이터 121개 생성: `~/.openclaw/workspace/reasoning-model/training_data.json`
- 분야: 수학, 과학, 프로그래밍, CS, 일상상식, 역사, 경제, 물리, 생물, 기술 등
- 형식: `{"instruction", "thinking", "output"}` — chain-of-thought
- Colab 노트북 업데이트: 파일 업로드 방식으로 변경 (training_data.json 업로드)

### 결과
- 121개 데이터 JSON 유효성 확인 완료
- 노트북 + 데이터 파일 준비 완료

### 다음 할 일
- [ ] Colab에 노트북 업로드 → T4 GPU 런타임 설정
- [x] 리셋 후 다른 구글 계정으로 Colab 재접속
- [x] 셀 하나씩 실행하며 진행

---

========================================
## 2026-02-16 03:01 — Colab 실행 가이드 저장 & 실행 시작
========================================

### 한 일
- Colab 실행 가이드 문서 저장: `~/Work/LLM/colab_guide.md`
- Colab 노트북 업로드 완료
- T4 GPU 런타임 설정 완료
- 셀 1 (Unsloth 설치) 실행 완료
- 셀 2 (모델 로드 + LoRA 설정) 실행 중

### 현재 상태
- Colab에서 Qwen2.5-3B-Instruct 모델 로딩 중
- 다음 단계: training_data.json 업로드 → 학습 실행

### 참고 문서
- `~/Work/LLM/colab_guide.md` — Colab 실행 완벽 가이드 (문제 해결, 체크리스트 포함)

---

========================================
## 2026-02-16 12:41 — Colab GPU 할당량 소진
========================================

### 상황
- Colab 무료 T4 GPU 할당량 초과로 연결 불가
- 어젯밤(02:54~04:50) 세션이 멈춘 상태로 GPU 시간 소모된 것으로 추정

---

========================================
## 2026-02-16 15:05 — 파인튜닝 완료 & 로컬 실행 성공! 🎉
========================================

### 한 일
- 다른 구글 계정으로 Colab 재접속, T4 GPU 할당 성공
- 학습 완료: Qwen2.5-3B-Instruct, 121개 데이터, 3에포크, LoRA r=16
- GGUF Q4_K_M 변환 완료: `Qwen2.5-3B-Instruct.Q4_K_M.gguf` (1.93GB)
- llama-server 실행: `localhost:8090`, Metal GPU 가속

### 테스트 결과
- "1부터 100까지의 합" → "5050" ✅ 정답
- "TCP와 UDP의 차이점" → 한국어로 정상 응답 ✅
- ⚠️ `<think>` 추론 태그는 아직 안 나옴 (데이터 부족)

---

========================================
## 2026-02-16 18:30 — 나무위키 크롤링 완료
========================================

### 한 일
- 나무위키 크롤러 생성: `~/Work/LLM/crawler/namu_crawler.py`
- 팰월드 1개, 오버워치 15개, 마인크래프트 12개 = 총 28개 문서

---

========================================
## 2026-02-16 18:55 — RAG 시스템 구축 & 웹 UI 완성
========================================

### 한 일
- `ingest.py`, `chat.py`, `web.py` 생성
- 28개 문서 → 3,617개 청크 벡터화 완료
- 웹 UI: `localhost:3333`

---

========================================
## 2026-02-16 19:35 — palworld.gg 크롤링 & GitHub 레포 생성
========================================

### 한 일
- palworld.gg 전용 크롤러: 57개 문서, 221,053자
- 벡터DB: 3,617 → 4,064개 청크
- GitHub: https://github.com/on1659/RadarCustomLLM_Game

---

========================================
## 2026-02-16 19:44 — 한국어 임베딩 모델 교체
========================================

### 문제
- 영어 임베딩 모델(`all-MiniLM-L6-v2`)로 한국어 검색 정확도 낮음
- 게임 간 문서 혼재

### 해결
- `jhgan/ko-sroberta-multitask` (한국어 특화) 로 교체
- 게임명 자동 감지 필터 추가

---

========================================
## 2026-02-16 20:15 — llm-server 스킬 생성
========================================

### 한 일
- `startllm` / `stopllm` / `statusllm` 명령어 스킬 생성
- 스킬 위치: `~/.openclaw/workspace/skills/llm-server/`

---

========================================
## 2026-02-16 20:56 — LLM 응답 품질 개선
========================================

### 수정 내용
- API: `/v1/chat/completions` → `/completion` (llama-server 네이티브)
- 컨텍스트: 1024 → 4096 토큰
- 검색 결과: 5개 → 3개, 청크당 500자 제한
- `n_predict`: 1024 → 256, `repeat_penalty`: 1.5
- stop 토큰 추가

---

========================================
## 2026-02-16 21:42 — 게임 역질문(disambiguation) 기능 추가
========================================

- 게임 키워드 없이 검색 시 여러 게임 감지되면 선택 버튼 표시
- "아누비스 알려줘" → [팰월드] [오버워치] 선택

---

========================================
## 2026-02-16 22:15 — 하이브리드 검색 도입 & Hallucination 방지 🔍
========================================

### 📚 개념 설명

#### 벡터 유사도 검색 (Vector Similarity Search)
텍스트를 숫자 벡터(임베딩)로 변환해서 "의미"가 비슷한 문서를 찾는 방식.

**어떻게 작동하나?**
1. 모든 문서를 임베딩 모델에 넣어서 고차원 벡터(예: 768차원)로 변환
2. 질문도 같은 모델로 벡터로 변환
3. 질문 벡터와 문서 벡터 사이의 거리(코사인 유사도)를 계산
4. 거리가 가까운(= 의미가 비슷한) 문서를 반환

**예시:**
```
"물 타입 최강 펠" → 벡터 [0.12, -0.34, 0.56, ...]
"수속성 가장 강한 팰월드 캐릭터" → 벡터 [0.11, -0.32, 0.55, ...]
→ 두 벡터의 코사인 유사도 = 0.95 (매우 유사!)
```

**장점:** "물"이랑 "수속성"처럼 다른 단어지만 같은 의미를 잡아냄
**단점:** "물 종결펠"에서 "종결"의 뉘앙스를 못 잡아서 엉뚱한 문서가 올 수 있음

#### BM25 키워드 검색
전통적인 키워드 매칭 검색. 질문에 포함된 **단어가 실제로 문서에 있는지** 확인.

**어떻게 작동하나?**
1. 모든 문서를 단어 단위로 분해 (토큰화)
2. 각 단어의 TF-IDF(문서 내 빈도 / 전체 문서 희귀도) 계산
3. 질문의 단어들이 문서에 얼마나 많이, 의미있게 등장하는지 점수화

**예시:**
```
질문: "물 종결펠"
→ "물"이라는 단어가 직접 포함된 문서를 높은 점수로
→ "종결"이 포함된 문서도 추가 점수
```

**장점:** 정확한 키워드가 포함된 문서를 확실하게 찾음
**단점:** "물"과 "수속성"이 같은 의미라는 걸 모름

#### 하이브리드 검색 = 벡터 + BM25 조합
두 방식을 합쳐서 서로의 단점을 보완:

```
[질문] → 벡터 검색 (의미 유사) → 결과 A
        → BM25 검색 (키워드 매칭) → 결과 B
        → A + B 합치기 (중복 제거) → 최종 결과
```

- 벡터가 놓친 "물"이라는 키워드를 BM25가 잡아줌
- BM25가 놓친 "수속성 = 물" 의미 연결을 벡터가 해줌

### 문제점 (이전 상태)
1. **벡터 유사도만 사용** → "물 종결펠"에서 엉뚱한 펠 문서(Azurobe, Suzaku-Aqua, Fuack) 검색됨
2. **LLM Hallucination** → 3B 모델이 context에 없는 정보를 지어냄 ("Cobalt Dusk(钴蓝暮光)" 등 창작)
3. **stop 조건 버그** → `"해당 정보가 없습니다."`가 stop 리스트에 있어서, 모델이 "모른다"고 답하려는 순간 문장이 잘림 → 어쩔 수 없이 아무 답이나 생성

### 수정 내용 (`rag/web.py`)

#### 1. 하이브리드 검색 도입
- **변경 전:** FAISS 벡터 유사도만 사용
- **변경 후:** 벡터 유사도 + BM25 키워드 매칭 조합
- `rank_bm25` 패키지 추가 설치
- 서버 시작 시 전체 문서에 대해 BM25 인덱스 구축 (4,064개)
- 검색 시 벡터 8개 + BM25 8개 결과를 합쳐서 중복 제거 후 상위 5개 사용
- 한국어 토크나이저: 정규식 기반 (`[가-힣a-zA-Z0-9]+`, 2글자 이상)

#### 2. 프롬프트 강화 (Hallucination 방지)
- **변경 전:** "참고 자료에 없으면 '해당 정보가 없습니다'라고 답하세요"
- **변경 후:** "참고 자료에 답이 없거나 불확실하면 **반드시** '해당 정보를 찾을 수 없습니다.'라고만 답하세요. **절대 추측하거나 지어내지 마세요.**"
- 규칙을 번호 리스트로 구조화 → 3B 모델이 더 잘 따름

#### 3. Temperature 낮춤
- **변경 전:** 0.3
- **변경 후:** 0.1
- 낮을수록 모델이 확신 높은 토큰만 선택 → 창작/Hallucination 감소

#### 4. Stop 조건 수정
- **변경 전:** `"해당 정보가 없습니다."` 포함 → "모른다" 답변이 잘림
- **변경 후:** 해당 항목 제거 → 모델이 "모릅니다" 답변을 완성할 수 있게 됨

#### 5. 검색 결과 확장
- 게임 필터 적용 시: 3개 → 5개 문서
- 청크당 최대: 500자 → 800자
- 더 많은 context로 정확도 향상

### 테스트 결과
- "팰월드 물 종결펠이 뭐야?" → Azurobe(물 드래곤)가 검색됨 ✅
- ⚠️ palworld.gg 크롤링 데이터에 인코딩 깨짐 있음 → 재크롤링 필요
- ⚠️ 3B 모델의 근본적 한계는 여전 → 학습 데이터 확장 or 더 큰 모델 필요

### 수정된 파일
- `rag/web.py` — 하이브리드 검색, 프롬프트 강화, temperature 조정, stop 수정

### 다음 할 일
- [ ] palworld.gg 크롤링 인코딩 수정 후 재크롤링
- [ ] 학습 데이터 500개로 확장 후 재학습
- [ ] 70B 모델 테스트 (128GB PC)
- [ ] 나무위키 팰월드 하위문서 크롤링 URL 수정

---

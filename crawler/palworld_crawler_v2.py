#!/usr/bin/env python3
"""palworld.gg í¬ë¡¤ëŸ¬ V2 â€” JavaScript ë Œë”ë§ ì§€ì›"""
import re
import json
import time
from pathlib import Path
from requests_html import HTMLSession
from bs4 import BeautifulSoup

OUTPUT_DIR = Path(__file__).parent / "data" / "palworld"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

PAGES = [
    ("pals", "https://palworld.gg/pals"),
    ("items", "https://palworld.gg/items"),
    ("structures", "https://palworld.gg/structures"),
    ("technology-tree", "https://palworld.gg/technology-tree"),
    ("breeding-calculator", "https://palworld.gg/breeding-calculator"),
    ("tier-list", "https://palworld.gg/tier-list"),
    ("capture-rate", "https://palworld.gg/capture-rate"),
]


def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬"""
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()


def fetch_page(session: HTMLSession, name: str, url: str) -> dict | None:
    """í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (JavaScript ë Œë”ë§)"""
    print(f"  ğŸ“¡ {name}...", end=" ", flush=True)
    
    try:
        resp = session.get(url, timeout=30)
        
        if resp.status_code != 200:
            print(f"âš ï¸ HTTP {resp.status_code}")
            return None
        
        # JavaScript ë Œë”ë§
        print(f"ğŸ¨ ë Œë”ë§...", end=" ", flush=True)
        resp.html.render(timeout=20, sleep=2)
        
        # BeautifulSoupìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ
        soup = BeautifulSoup(resp.html.html, 'lxml')
        
        # ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼ ì œê±°
        for tag in soup.find_all(['script', 'style', 'nav', 'footer', 'header']):
            tag.decompose()
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        text = soup.get_text(separator='\n', strip=True)
        text = clean_text(text)
        
        if len(text) > 200:
            filepath = OUTPUT_DIR / f"palworld_gg_{name}.txt"
            filepath.write_text(text, encoding='utf-8')
            print(f"âœ… ({len(text):,}ì)")
            return {"title": name, "length": len(text)}
        else:
            print("â­ï¸ ë‚´ìš© ë¶€ì¡±")
            return None
            
    except Exception as e:
        print(f"âŒ {e}")
        return None


def fetch_individual_pals(session: HTMLSession):
    """ê°œë³„ íŒ° í˜ì´ì§€ í¬ë¡¤ë§"""
    print("\n  ğŸ¾ ê°œë³„ íŒ° í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
    
    try:
        resp = session.get("https://palworld.gg/pals", timeout=30)
        resp.html.render(timeout=20, sleep=2)
        
        # íŒ° ë§í¬ ì¶”ì¶œ
        soup = BeautifulSoup(resp.html.html, 'lxml')
        pal_links = []
        
        for a in soup.find_all('a', href=True):
            href = a['href']
            if '/pal/' in href:
                pal_name = href.split('/pal/')[-1].split('?')[0].split('#')[0]
                if pal_name and pal_name not in pal_links:
                    pal_links.append(pal_name)
        
        pal_links = pal_links[:50]  # ìµœëŒ€ 50ê°œ
        print(f"  â†’ {len(pal_links)}ê°œ íŒ° ë°œê²¬\n")
        
        results = []
        for i, pal in enumerate(pal_links, 1):
            print(f"    [{i}/{len(pal_links)}] {pal}...", end=" ", flush=True)
            
            try:
                r = session.get(f"https://palworld.gg/pal/{pal}", timeout=30)
                if r.status_code == 200:
                    r.html.render(timeout=20, sleep=1)
                    soup = BeautifulSoup(r.html.html, 'lxml')
                    
                    # ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼ ì œê±°
                    for tag in soup.find_all(['script', 'style', 'nav', 'footer', 'header']):
                        tag.decompose()
                    
                    text = soup.get_text(separator='\n', strip=True)
                    text = clean_text(text)
                    
                    if len(text) > 200:
                        filepath = OUTPUT_DIR / f"pal_{pal}.txt"
                        filepath.write_text(text, encoding='utf-8')
                        results.append({"title": f"pal_{pal}", "length": len(text)})
                        print(f"âœ… ({len(text):,}ì)")
                    else:
                        print("â­ï¸")
                else:
                    print(f"âš ï¸ {r.status_code}")
            except Exception as e:
                print(f"âŒ {e}")
            
            time.sleep(2)
        
        return results
        
    except Exception as e:
        print(f"  âŒ íŒ° ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []


def main():
    print("ğŸ® palworld.gg í¬ë¡¤ëŸ¬ V2 (JavaScript ë Œë”ë§ ì§€ì›)")
    print(f"ğŸ“ ì €ì¥: {OUTPUT_DIR}")
    print("â±ï¸  ë Œë”ë§ìœ¼ë¡œ ì¸í•´ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤...\n")
    
    session = HTMLSession()
    results = []
    
    # ë©”ì¸ í˜ì´ì§€ë“¤
    print("ğŸ“„ ë©”ì¸ í˜ì´ì§€:")
    for name, url in PAGES:
        r = fetch_page(session, name, url)
        if r:
            results.append(r)
        time.sleep(3)
    
    # ê°œë³„ íŒ°
    pal_results = fetch_individual_pals(session)
    results.extend(pal_results)
    
    session.close()
    
    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    meta_path = OUTPUT_DIR / "_meta.json"
    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    total = sum(r["length"] for r in results)
    print(f"\nğŸ“Š ì™„ë£Œ: {len(results)}ê°œ ë¬¸ì„œ, ì´ {total:,}ì")
    print("\nâœ… ì™„ë£Œ! ì´ì œ ingest.pyë¡œ ë²¡í„°DBë¥¼ ìƒì„±í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()

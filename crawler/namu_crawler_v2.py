#!/usr/bin/env python3
"""
ë‚˜ë¬´ìœ„í‚¤ í¬ë¡¤ëŸ¬ V2 â€” JavaScript ë Œë”ë§ ì§€ì›
ì‚¬ìš©ë²•: python3 namu_crawler_v2.py
"""

import re
import json
import time
from pathlib import Path
from urllib.parse import quote
from requests_html import HTMLSession
from bs4 import BeautifulSoup

OUTPUT_DIR = Path(__file__).parent / "data"
OUTPUT_DIR.mkdir(exist_ok=True)

# í¬ë¡¤ë§í•  ë¬¸ì„œ ëª©ë¡
PAGES = {
    "palworld": [
        "íŒ°ì›”ë“œ",
        "íŒ°ì›”ë“œ/íŒ°",
        "íŒ°ì›”ë“œ/ë„ê°",
        "íŒ°ì›”ë“œ/ë³´ìŠ¤",
        "íŒ°ì›”ë“œ/ê¸°ìˆ ",
        "íŒ°ì›”ë“œ/ê±°ì ",
        "íŒ°ì›”ë“œ/ì•„ì´í…œ",
        "íŒ°ì›”ë“œ/ë¬´ê¸°",
        "íŒ°ì›”ë“œ/ë°©ì–´êµ¬",
        "íŒ°ì›”ë“œ/ê±´ì¶•",
        "íŒ°ì›”ë“œ/íƒˆê²ƒ",
        "íŒ°ì›”ë“œ/ë²ˆì‹",
        "íŒ°ì›”ë“œ/íŒ",
        "íœí‚¹(íŒ°ì›”ë“œ)",
    ],
    "overwatch": [
        "ì˜¤ë²„ì›Œì¹˜ 2",
        "ì˜¤ë²„ì›Œì¹˜/ì˜ì›…",
        "ì˜¤ë²„ì›Œì¹˜/ì˜ì›…/ëŒê²©",
        "ì˜¤ë²„ì›Œì¹˜/ì˜ì›…/í”¼í•´",
        "ì˜¤ë²„ì›Œì¹˜/ì˜ì›…/ì§€ì›",
        "ì˜¤ë²„ì›Œì¹˜/ê²Œì„ ëª¨ë“œ",
        "ì˜¤ë²„ì›Œì¹˜/ì „ì¥",
        "ì˜¤ë²„ì›Œì¹˜/ì•„ì´í…œ",
        "ì˜¤ë²„ì›Œì¹˜/ê²½ìŸì „",
        # ê°œë³„ ì˜ì›…
        "íŠ¸ë ˆì´ì„œ(ì˜¤ë²„ì›Œì¹˜)",
        "ê²ì§€(ì˜¤ë²„ì›Œì¹˜)",
        "ë¦¬í¼(ì˜¤ë²„ì›Œì¹˜)",
        "ì†”ì €: 76",
        "íŒŒë¼(ì˜¤ë²„ì›Œì¹˜)",
        "ì•„ë‚˜(ì˜¤ë²„ì›Œì¹˜)",
        "ë£¨ì‹œìš°(ì˜¤ë²„ì›Œì¹˜)",
        "ë¨¸ì‹œ(ì˜¤ë²„ì›Œì¹˜)",
        "ë¼ì¸í•˜ë¥´íŠ¸(ì˜¤ë²„ì›Œì¹˜)",
        "ë””ë°”(ì˜¤ë²„ì›Œì¹˜)",
        "ìœ„ë„ìš°ë©”ì´ì»¤(ì˜¤ë²„ì›Œì¹˜)",
        "í•œì¡°(ì˜¤ë²„ì›Œì¹˜)",
        "ì •í¬ë«(ì˜¤ë²„ì›Œì¹˜)",
        "ë©”ì´(ì˜¤ë²„ì›Œì¹˜)",
        "ë°”ìŠ¤í‹°ì˜¨(ì˜¤ë²„ì›Œì¹˜)",
    ],
    "minecraft": [
        "ë§ˆì¸í¬ë˜í”„íŠ¸",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ì•„ì´í…œ",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ë¸”ë¡",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ëª¹",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ë°”ì´ì˜´",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ì¸ì±ˆíŠ¸",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ì–‘ì¡°",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ë ˆë“œìŠ¤í†¤",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ì—”ë” ë“œë˜ê³¤",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ìœ„ë”",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ë„¤ë”",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ë„¤ë”ë¼ì´íŠ¸",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ì—”ë“œ",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ë§ˆì„",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ë†ì‚¬",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ì¡°í•©ë²•",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/íŒ",
        "ìœ„ë”",
        "ë„¤ë”ë¼ì´íŠ¸",
    ],
}


def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬"""
    # ë‚˜ë¬´ìœ„í‚¤ ë¬¸ë²• ì •ë¦¬
    text = re.sub(r'\[\[([^\]|]+)\|([^\]]+)\]\]', r'\2', text)  # [[ë§í¬|í…ìŠ¤íŠ¸]] â†’ í…ìŠ¤íŠ¸
    text = re.sub(r'\[\[([^\]]+)\]\]', r'\1', text)  # [[ë§í¬]] â†’ ë§í¬
    text = re.sub(r'\{\{[^}]*\}\}', '', text)  # {{ë§¤í¬ë¡œ}} ì œê±°
    # ì—¬ëŸ¬ ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()


def fetch_namu_page(session: HTMLSession, title: str) -> str | None:
    """ë‚˜ë¬´ìœ„í‚¤ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (JavaScript ë Œë”ë§)"""
    encoded = quote(title, safe='')
    url = f"https://namu.wiki/w/{encoded}"
    
    try:
        print(f"    ğŸ“¡ ìš”ì²­ ì¤‘...", end=" ", flush=True)
        resp = session.get(url, timeout=30)
        
        if resp.status_code != 200:
            print(f"âš ï¸ HTTP {resp.status_code}")
            return None
        
        # JavaScript ë Œë”ë§ (ì‹œê°„ ì†Œìš”)
        print(f"ğŸ¨ ë Œë”ë§...", end=" ", flush=True)
        resp.html.render(timeout=20, sleep=2)
        
        # BeautifulSoupìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ
        soup = BeautifulSoup(resp.html.html, 'lxml')
        
        # ë‚˜ë¬´ìœ„í‚¤ ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
        content_divs = soup.find_all(['div', 'article'], class_=lambda x: x and any(
            keyword in str(x).lower() for keyword in ['wiki-content', 'wiki-article', 'document', 'content']
        ))
        
        if not content_divs:
            # í´ë˜ìŠ¤ ì—†ì´ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            content_divs = [soup.body] if soup.body else []
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = []
        for div in content_divs:
            # ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
            for tag in div.find_all(['script', 'style', 'nav', 'footer', 'header']):
                tag.decompose()
            
            text = div.get_text(separator='\n', strip=True)
            if text and len(text) > 100:
                texts.append(text)
        
        if not texts:
            print("â­ï¸ ë³¸ë¬¸ ì—†ìŒ")
            return None
        
        # ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ì„ íƒ
        final_text = max(texts, key=len)
        final_text = clean_text(final_text)
        
        return final_text if len(final_text) > 100 else None
        
    except Exception as e:
        print(f"âŒ {e}")
        return None


def crawl_game(game: str, pages: list[str]):
    """ê²Œì„ë³„ ë¬¸ì„œ í¬ë¡¤ë§"""
    game_dir = OUTPUT_DIR / game
    game_dir.mkdir(exist_ok=True)
    
    results = []
    session = HTMLSession()
    
    print(f"\n{'='*60}")
    print(f"ğŸ® {game.upper()} í¬ë¡¤ë§ ì‹œì‘ ({len(pages)}ê°œ ë¬¸ì„œ)")
    print(f"{'='*60}")
    
    for i, title in enumerate(pages, 1):
        print(f"  [{i}/{len(pages)}] {title}")
        
        text = fetch_namu_page(session, title)
        
        if text and len(text) > 100:
            # ê°œë³„ íŒŒì¼ ì €ì¥
            safe_name = re.sub(r'[/\\:*?"<>|]', '_', title)
            filepath = game_dir / f"{safe_name}.txt"
            filepath.write_text(text, encoding='utf-8')
            
            results.append({
                "title": title,
                "file": str(filepath),
                "length": len(text),
            })
            print(f"    âœ… ì €ì¥ ì™„ë£Œ ({len(text):,}ì)\n")
        else:
            print(f"    â­ï¸ ìŠ¤í‚µ (ë‚´ìš© ë¶€ì¡±)\n")
        
        time.sleep(3)  # 3ì´ˆ ëŒ€ê¸° (rate limit)
    
    session.close()
    
    # ê²Œì„ë³„ ë©”íƒ€ë°ì´í„° ì €ì¥
    meta_path = game_dir / "_meta.json"
    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    total_chars = sum(r["length"] for r in results)
    print(f"\nğŸ“Š {game}: {len(results)}ê°œ ë¬¸ì„œ, ì´ {total_chars:,}ì ì €ì¥")
    
    return results


def main():
    print("ğŸ•·ï¸ ë‚˜ë¬´ìœ„í‚¤ ê²Œì„ í¬ë¡¤ëŸ¬ V2 (JavaScript ë Œë”ë§ ì§€ì›)")
    print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {OUTPUT_DIR}")
    print("â±ï¸  ë Œë”ë§ìœ¼ë¡œ ì¸í•´ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤...\n")
    
    all_results = {}
    for game, pages in PAGES.items():
        all_results[game] = crawl_game(game, pages)
    
    # ì „ì²´ ìš”ì•½
    print(f"\n{'='*60}")
    print("ğŸ“‹ í¬ë¡¤ë§ ì™„ë£Œ ìš”ì•½")
    print(f"{'='*60}")
    
    total_docs = 0
    total_chars = 0
    for game, results in all_results.items():
        docs = len(results)
        chars = sum(r["length"] for r in results)
        total_docs += docs
        total_chars += chars
        print(f"  ğŸ® {game}: {docs}ê°œ ë¬¸ì„œ, {chars:,}ì")
    
    print(f"\n  ì´í•©: {total_docs}ê°œ ë¬¸ì„œ, {total_chars:,}ì")
    print(f"  ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}")
    print("\nâœ… ì™„ë£Œ! ì´ì œ ingest.pyë¡œ ë²¡í„°DBë¥¼ ìƒì„±í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()

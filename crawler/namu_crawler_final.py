#!/usr/bin/env python3
"""
ë‚˜ë¬´ìœ„í‚¤ í¬ë¡¤ëŸ¬ (Playwright ê¸°ë°˜)
ì‚¬ìš©ë²•: python3 namu_crawler_final.py
"""

import re
import json
import time
from pathlib import Path
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

OUTPUT_DIR = Path(__file__).parent / "data"
OUTPUT_DIR.mkdir(exist_ok=True)

# í¬ë¡¤ë§í•  ë¬¸ì„œ ëª©ë¡
PAGES = {
    "overwatch": [
        "ì˜¤ë²„ì›Œì¹˜ 2",
        "ì˜¤ë²„ì›Œì¹˜/ì˜ì›…",
        "ì˜¤ë²„ì›Œì¹˜/ê²Œì„ ëª¨ë“œ",
        "ì˜¤ë²„ì›Œì¹˜/ì „ì¥",
        "ì˜¤ë²„ì›Œì¹˜/ê²½ìŸì „",
        # ê°œë³„ ì˜ì›…
        "íŠ¸ë ˆì´ì„œ(ì˜¤ë²„ì›Œì¹˜)",
        "ê²ì§€(ì˜¤ë²„ì›Œì¹˜)",
        "ë¦¬í¼(ì˜¤ë²„ì›Œì¹˜)",
        "ì†”ì €: 76",
        "ì•„ë‚˜(ì˜¤ë²„ì›Œì¹˜)",
        "ë£¨ì‹œìš°(ì˜¤ë²„ì›Œì¹˜)",
        "ë¨¸ì‹œ(ì˜¤ë²„ì›Œì¹˜)",
        "ë¼ì¸í•˜ë¥´íŠ¸(ì˜¤ë²„ì›Œì¹˜)",
        "ë””ë°”(ì˜¤ë²„ì›Œì¹˜)",
        "í•œì¡°(ì˜¤ë²„ì›Œì¹˜)",
        "ì •í¬ë«(ì˜¤ë²„ì›Œì¹˜)",
        "ë©”ì´(ì˜¤ë²„ì›Œì¹˜)",
    ],
    "minecraft": [
        "ë§ˆì¸í¬ë˜í”„íŠ¸",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ì•„ì´í…œ",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ë¸”ë¡",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ëª¹",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ë°”ì´ì˜´",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ë§ˆë²• ë¶€ì—¬",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ì–‘ì¡°",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ë ˆë“œìŠ¤í†¤",
        "ìœ„ë”",
        "ì—”ë” ë“œë˜ê³¤",
        "ë„¤ë”ë¼ì´íŠ¸",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ë§ˆì„",
        "ë§ˆì¸í¬ë˜í”„íŠ¸/ë†ì‚¬",
    ],
}


def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬"""
    text = re.sub(r'\[\[([^\]|]+)\|([^\]]+)\]\]', r'\2', text)
    text = re.sub(r'\[\[([^\]]+)\]\]', r'\1', text)
    text = re.sub(r'\{\{[^}]*\}\}', '', text)
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()


def fetch_namu_page(page, title: str) -> str | None:
    """ë‚˜ë¬´ìœ„í‚¤ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°"""
    url = f"https://namu.wiki/w/{title}"
    
    try:
        print(f"    ğŸ“¡ ë¡œë“œ...", end=" ", flush=True)
        page.goto(url, wait_until='networkidle', timeout=30000)
        print(f"ë Œë”ë§...", end=" ", flush=True)
        page.wait_for_timeout(2000)  # 2ì´ˆ ëŒ€ê¸°
        
        html_content = page.content()
        soup = BeautifulSoup(html_content, 'lxml')
        
        # ë³¸ë¬¸ ì°¾ê¸°
        content_divs = soup.find_all(['div', 'article'], class_=lambda x: x and any(
            keyword in str(x).lower() for keyword in ['wiki-content', 'wiki-article', 'document', 'content']
        ))
        
        if not content_divs:
            content_divs = [soup.body] if soup.body else []
        
        texts = []
        for div in content_divs:
            for tag in div.find_all(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                tag.decompose()
            
            text = div.get_text(separator='\n', strip=True)
            if text and len(text) > 100:
                texts.append(text)
        
        final_text = max(texts, key=len) if texts else ""
        final_text = clean_text(final_text)
        
        if len(final_text) > 100:
            print(f"âœ… ({len(final_text):,}ì)")
            return final_text
        else:
            print("â­ï¸ ë‚´ìš© ë¶€ì¡±")
            return None
            
    except Exception as e:
        print(f"âŒ {e}")
        return None


def crawl_game(game: str, pages: list[str]):
    """ê²Œì„ë³„ ë¬¸ì„œ í¬ë¡¤ë§"""
    game_dir = OUTPUT_DIR / game
    game_dir.mkdir(exist_ok=True)
    
    results = []
    
    print(f"\n{'='*60}")
    print(f"ğŸ® {game.upper()} í¬ë¡¤ë§ ì‹œì‘ ({len(pages)}ê°œ ë¬¸ì„œ)")
    print(f"{'='*60}")
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        for i, title in enumerate(pages, 1):
            print(f"  [{i}/{len(pages)}] {title}")
            
            text = fetch_namu_page(page, title)
            
            if text:
                # ê°œë³„ íŒŒì¼ ì €ì¥
                safe_name = re.sub(r'[/\\:*?"<>|]', '_', title)
                filepath = game_dir / f"{safe_name}.txt"
                filepath.write_text(text, encoding='utf-8')
                
                results.append({
                    "title": title,
                    "file": str(filepath),
                    "length": len(text),
                })
            
            time.sleep(2)  # Rate limit
        
        browser.close()
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    meta_path = game_dir / "_meta.json"
    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    total_chars = sum(r["length"] for r in results)
    print(f"\nğŸ“Š {game}: {len(results)}ê°œ ë¬¸ì„œ, ì´ {total_chars:,}ì ì €ì¥")
    
    return results


def main():
    print("ğŸ•·ï¸ ë‚˜ë¬´ìœ„í‚¤ í¬ë¡¤ëŸ¬ (Playwright)")
    print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {OUTPUT_DIR}\n")
    
    all_results = {}
    for game, pages in PAGES.items():
        all_results[game] = crawl_game(game, pages)
    
    # ì „ì²´ ìš”ì•½
    print(f"\n{'='*60}")
    print("ğŸ“‹ í¬ë¡¤ë§ ì™„ë£Œ ìš”ì•½")
    print(f"{'='*60}")
    
    total_docs = 0
    total_chars = 0
    for game, results in all_results.items():
        docs = len(results)
        chars = sum(r["length"] for r in results)
        total_docs += docs
        total_chars += chars
        print(f"  ğŸ® {game}: {docs}ê°œ ë¬¸ì„œ, {chars:,}ì")
    
    print(f"\n  ì´í•©: {total_docs}ê°œ ë¬¸ì„œ, {total_chars:,}ì")
    print(f"  ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}")
    print("\nâœ… ì™„ë£Œ! ì´ì œ ingest.pyë¡œ ë²¡í„°DBë¥¼ ìƒì„±í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()

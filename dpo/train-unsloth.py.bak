#!/usr/bin/env python3
"""DPO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (unsloth + LoRA)
chosen/rejected í˜ì–´ë¡œ ëª¨ë¸ í•™ìŠµ
"""

import json
from pathlib import Path
from datasets import Dataset
import torch

# ì„¤ì •
DATASET_DIR = Path(__file__).parent / "dataset"
CHOSEN_FILE = DATASET_DIR / "chosen.jsonl"
OUTPUT_DIR = Path(__file__).parent / "models"
OUTPUT_DIR.mkdir(exist_ok=True)

# ëª¨ë¸ ê²½ë¡œ
BASE_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # HuggingFace ëª¨ë¸ëª…
GGUF_MODEL = Path.home() / "Work/LLM/models/Qwen2.5-7B-Instruct-Q4_K_M.gguf"


def load_dataset():
    """chosen.jsonl ë¡œë“œ"""
    data = []
    with open(CHOSEN_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    
    print(f"ğŸ“š ë°ì´í„°ì…‹ ë¡œë“œ: {len(data)}ê°œ í˜ì–´")
    
    if len(data) < 10:
        print("âš ï¸  ê²½ê³ : ë°ì´í„°ê°€ 10ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. ìµœì†Œ 50ê°œ ì´ìƒ ê¶Œì¥!")
        return None
    
    return data


def prepare_dpo_dataset(data):
    """DPO í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    formatted = []
    
    for item in data:
        formatted.append({
            "prompt": f"<|im_start|>user\n{item['question']}<|im_end|>\n<|im_start|>assistant\n",
            "chosen": item["chosen"],
            "rejected": item["rejected"]
        })
    
    return Dataset.from_list(formatted)


def train_dpo():
    """DPO í•™ìŠµ ì‹¤í–‰"""
    print("\nğŸš€ DPO í•™ìŠµ ì‹œì‘\n")
    
    # ë°ì´í„° ë¡œë“œ
    data = load_dataset()
    if not data:
        return
    
    dataset = prepare_dpo_dataset(data)
    print(f"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: {len(dataset)}ê°œ")
    
    try:
        print("\nğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì¤‘...")
        from unsloth import FastLanguageModel, is_bfloat16_supported
        from trl import DPOTrainer, DPOConfig
        
        # ëª¨ë¸ ë¡œë“œ
        print(f"\nğŸ”§ ëª¨ë¸ ë¡œë“œ: {BASE_MODEL}")
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=BASE_MODEL,
            max_seq_length=2048,
            dtype=None,  # ìë™ ì„ íƒ
            load_in_4bit=True,  # 4bit ì–‘ìí™”
        )
        
        # LoRA ì–´ëŒ‘í„° ì¶”ê°€
        print("\nğŸ¯ LoRA ì–´ëŒ‘í„° ì„¤ì •")
        model = FastLanguageModel.get_peft_model(
            model,
            r=16,  # LoRA rank
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", 
                          "gate_proj", "up_proj", "down_proj"],
            lora_alpha=16,
            lora_dropout=0.05,
            bias="none",
            use_gradient_checkpointing="unsloth",
            random_state=42,
        )
        
        # DPO í•™ìŠµ ì„¤ì •
        print("\nâš™ï¸  í•™ìŠµ ì„¤ì •")
        training_args = DPOConfig(
            output_dir=str(OUTPUT_DIR / "checkpoints"),
            per_device_train_batch_size=1,
            gradient_accumulation_steps=4,
            num_train_epochs=3,
            learning_rate=5e-5,
            fp16=not is_bfloat16_supported(),
            bf16=is_bfloat16_supported(),
            logging_steps=10,
            save_steps=100,
            optim="adamw_8bit",
            warmup_ratio=0.1,
            max_length=2048,
            max_prompt_length=1024,
            beta=0.1,  # DPO beta
        )
        
        trainer = DPOTrainer(
            model=model,
            args=training_args,
            train_dataset=dataset,
            tokenizer=tokenizer,
        )
        
        # í•™ìŠµ ì‹œì‘
        print("\nğŸ”¥ í•™ìŠµ ì‹œì‘!")
        print(f"  - ë°ì´í„°: {len(dataset)}ê°œ")
        print(f"  - ì—í­: {training_args.num_train_epochs}")
        print(f"  - ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size}")
        print(f"  - ì˜ˆìƒ ì‹œê°„: ~{len(dataset) * training_args.num_train_epochs // 10} ë¶„\n")
        
        trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        model.save_pretrained(str(OUTPUT_DIR / "lora_adapter"))
        tokenizer.save_pretrained(str(OUTPUT_DIR / "lora_adapter"))
        
        print("\nâœ… í•™ìŠµ ì™„ë£Œ!")
        print(f"\nì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR / 'lora_adapter'}")
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("  1. python dpo/convert-to-gguf.py  # GGUF ë³€í™˜")
        print("  2. llmcron restart                # ì„œë²„ ì¬ì‹œì‘")
    
    except ImportError as e:
        print(f"\nâŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”:")
        print(f"   pip install unsloth trl datasets peft bitsandbytes accelerate")
        print(f"\nìƒì„¸ ì˜¤ë¥˜: {e}")
    
    except Exception as e:
        print(f"\nâŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    if not torch.cuda.is_available() and not torch.backends.mps.is_available():
        print("âš ï¸  ê²½ê³ : GPU/MPSë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   CPUë¡œ í•™ìŠµí•˜ë©´ ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        choice = input("\nê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if choice != 'y':
            exit(0)
    
    train_dpo()

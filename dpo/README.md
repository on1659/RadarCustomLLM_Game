# DPO (Direct Preference Optimization) í•™ìŠµ íŒŒì´í”„ë¼ì¸

Qwen2.5-7B ëª¨ë¸ì„ ê²Œì„ ìœ„í‚¤ RAGì— íŠ¹í™”ë˜ë„ë¡ fine-tuningí•˜ëŠ” ì‹œìŠ¤í…œ.

## ğŸ“‹ ê°œìš”

**ëª©í‘œ:** QA í…ŒìŠ¤íŠ¸ì—ì„œ í‹€ë¦° ë‹µë³€(rejected)ì„ ì˜¬ë°”ë¥¸ ë‹µë³€(chosen)ìœ¼ë¡œ í•™ìŠµì‹œì¼œ ì •í™•ë„ í–¥ìƒ.

**ë°©ë²•:** DPO (ì‚¬ëŒì˜ ì„ í˜¸ë„ í•™ìŠµ) + LoRA (íš¨ìœ¨ì  fine-tuning)

**í•„ìš” ë°ì´í„°:** 500ìŒ ê¶Œì¥ (ìµœì†Œ 50ìŒ)

## ğŸ› ï¸ ì„¤ì¹˜

### 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬

```bash
cd ~/Work/LLM
pip install unsloth trl datasets peft bitsandbytes accelerate transformers torch
```

### 2. llama.cpp (GGUF ë³€í™˜ìš©)

```bash
cd ~/Work/LLM
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make
```

## ğŸš€ ì›Œí¬í”Œë¡œìš°

### 1ï¸âƒ£ ë°ì´í„° ìˆ˜ì§‘ (ìë™)

```bash
python dpo/collect-data.py
```

**ë™ì‘:**
- ìµœê·¼ 7ì¼ê°„ QA ë¡œê·¸ì—ì„œ ì •í™•ë„ 70% ì´í•˜ ë‹µë³€ ìˆ˜ì§‘
- `dataset/rejected.jsonl` ìƒì„±
- `dataset/pending.json` í ìƒì„± (ìˆ˜ë™ ìˆ˜ì • ëŒ€ê¸°)

**ì¶œë ¥ ì˜ˆì‹œ:**
```
ğŸ“– 2026-02-22.md íŒŒì‹± ì¤‘...
âœ… 12ê°œ ìƒˆ rejected ë‹µë³€ ìˆ˜ì§‘ (ì´ 45ê°œ)
ğŸ“‹ ìˆ˜ë™ ìˆ˜ì • ëŒ€ê¸° ì¤‘: 45ê°œ

ğŸ“Š DPO ë°ì´í„°ì…‹ í˜„í™©:
  - Rejected: 45ê°œ
  - Chosen: 0ê°œ
  - Pending (ìˆ˜ì • ëŒ€ê¸°): 45ê°œ
  - í•™ìŠµ ê°€ëŠ¥ í˜ì–´: 0ê°œ

ğŸ¯ ê¶Œì¥ í•™ìŠµëŸ‰: 500ìŒ (í˜„ì¬ ì§„í–‰ë¥ : 0.0%)
```

---

### 2ï¸âƒ£ ìˆ˜ë™ ë‹µë³€ ì‘ì„± (ì¸ê°„ í”¼ë“œë°±)

```bash
python dpo/manual-fix.py
```

**ëŒ€í™”í˜• ëª¨ë“œ:**
```
[1/45]

â“ ì§ˆë¬¸: ë§ˆì¸í¬ë˜í”„íŠ¸ ì—”ë”ë“œë˜ê³¤
âŒ ê¸°ì¡´ ë‹µë³€ (ì •í™•ë„ 45%):
ì—”ë”ë“œë˜ê³¤ì€ ë§ˆì¸í¬ë˜í”„íŠ¸ì˜ ëª¬ìŠ¤í„°ì…ë‹ˆë‹¤...

ì˜µì…˜:
  1. ì˜¬ë°”ë¥¸ ë‹µë³€ ì‘ì„±
  2. ê±´ë„ˆë›°ê¸° (skip)
  3. ì‚­ì œ (ì´ ì§ˆë¬¸ ì œì™¸)
  q. ì¢…ë£Œ

ì„ íƒ: 1

âœï¸  ì˜¬ë°”ë¥¸ ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš” (ì—¬ëŸ¬ ì¤„ ì…ë ¥ ê°€ëŠ¥, ë¹ˆ ì¤„ + Enterë¡œ ì™„ë£Œ):
ì—”ë”ë“œë˜ê³¤ì€ ë§ˆì¸í¬ë˜í”„íŠ¸ì˜ ìµœì¢… ë³´ìŠ¤ ëª¬ìŠ¤í„°ì…ë‹ˆë‹¤.
ì—”ë“œ ì°¨ì›ì—ì„œ ë§Œë‚  ìˆ˜ ìˆìœ¼ë©°, ì—”ë“œ í¬ë¦¬ìŠ¤íƒˆì„ íŒŒê´´í•˜ë©´ì„œ ê³µëµí•´ì•¼ í•©ë‹ˆë‹¤.
ì²˜ì¹˜í•˜ë©´ ëŒ€ëŸ‰ì˜ ê²½í—˜ì¹˜ì™€ ë“œë˜ê³¤ ì•Œì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
[ë¹ˆ ì¤„]

âœ… ì €ì¥ ì™„ë£Œ!
```

**ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°:**
- JSON íŒŒì¼ë¡œ ëŒ€ëŸ‰ import ê°€ëŠ¥
- í˜•ì‹: `[{"question": "...", "rejected": "...", "chosen": "..."}]`

---

### 3ï¸âƒ£ DPO í•™ìŠµ

```bash
python dpo/train.py
```

**ìš”êµ¬ì‚¬í•­:**
- ìµœì†Œ 10ê°œ í˜ì–´ (ê¶Œì¥ 50+)
- GPU/MPS ê¶Œì¥ (CPUë„ ê°€ëŠ¥í•˜ë‚˜ ëŠë¦¼)
- M4 16GB: LoRA 4bitë¡œ í•™ìŠµ ê°€ëŠ¥

**í•™ìŠµ ê³¼ì •:**
```
ğŸš€ DPO í•™ìŠµ ì‹œì‘

ğŸ“š ë°ì´í„°ì…‹ ë¡œë“œ: 50ê°œ í˜ì–´
âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: 50ê°œ

ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì¤‘...
ğŸ”§ ëª¨ë¸ ë¡œë“œ: Qwen/Qwen2.5-7B-Instruct
ğŸ¯ LoRA ì–´ëŒ‘í„° ì„¤ì •

âš™ï¸  í•™ìŠµ ì„¤ì •
ğŸ”¥ í•™ìŠµ ì‹œì‘!
  - ë°ì´í„°: 50ê°œ
  - ì—í­: 3
  - ë°°ì¹˜ í¬ê¸°: 1
  - ì˜ˆìƒ ì‹œê°„: ~15 ë¶„

[ì—í­ 1/3] Step 10/150 | Loss: 0.523
[ì—í­ 2/3] Step 60/150 | Loss: 0.412
[ì—í­ 3/3] Step 150/150 | Loss: 0.387

ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...
âœ… í•™ìŠµ ì™„ë£Œ!

ì €ì¥ ìœ„ì¹˜: ~/Work/LLM/dpo/models/lora_adapter
```

**í•™ìŠµ íŒŒë¼ë¯¸í„°:**
- **LoRA rank:** 16
- **Learning rate:** 5e-5
- **Beta (DPO):** 0.1
- **Batch size:** 1 (gradient accumulation 4)
- **Epochs:** 3

---

### 4ï¸âƒ£ GGUF ë³€í™˜ ë° ë°°í¬

```bash
python dpo/convert-to-gguf.py
```

**ê³¼ì •:**
1. LoRA ì–´ëŒ‘í„°ë¥¼ ê¸°ë³¸ ëª¨ë¸ì— ë³‘í•©
2. HuggingFace â†’ FP16 GGUF
3. FP16 â†’ Q4_K_M ì–‘ìí™”
4. `~/Work/LLM/models/` ì— ë°°í¬

**ì¶œë ¥ ì˜ˆì‹œ:**
```
ğŸ”„ LoRA â†’ GGUF ë³€í™˜ íŒŒì´í”„ë¼ì¸

ğŸ”„ LoRA ì–´ëŒ‘í„° ë³‘í•© ì¤‘...
âœ… ë³‘í•© ì™„ë£Œ: ~/Work/LLM/dpo/models/merged_model

ğŸ”§ GGUF ë³€í™˜ ì¤‘...
ì‹¤í–‰: python convert_hf_to_gguf.py ...
âœ… FP16 GGUF ìƒì„±: model-f16.gguf

ğŸ”§ ì–‘ìí™” ì¤‘ (Q4_K_M)...
ì‹¤í–‰: llama-quantize model-f16.gguf model-Q4_K_M.gguf Q4_K_M
âœ… ì–‘ìí™” ì™„ë£Œ: model-Q4_K_M.gguf

ğŸš€ ì„œë²„ ë°°í¬
âœ… ë°°í¬ ì™„ë£Œ: ~/Work/LLM/models/Qwen2.5-7B-DPO-Q4_K_M.gguf
```

---

### 5ï¸âƒ£ ì„œë²„ ì¬ì‹œì‘

```bash
# ìë™ ì¬ì‹œì‘ (ê¶Œì¥)
llmcron restart

# ìˆ˜ë™ ì¬ì‹œì‘
pkill llama-server
cd ~/Work/LLM
nohup ./build/bin/llama-server \
  -m models/Qwen2.5-7B-DPO-Q4_K_M.gguf \
  -c 8192 --port 8090 > llama-server.log 2>&1 &
```

---

## ğŸ“‚ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
dpo/
â”œâ”€â”€ README.md              # ì´ íŒŒì¼
â”œâ”€â”€ collect-data.py        # 1ï¸âƒ£ ë°ì´í„° ìˆ˜ì§‘
â”œâ”€â”€ manual-fix.py          # 2ï¸âƒ£ ìˆ˜ë™ ë‹µë³€ ì‘ì„±
â”œâ”€â”€ train.py               # 3ï¸âƒ£ DPO í•™ìŠµ
â”œâ”€â”€ convert-to-gguf.py     # 4ï¸âƒ£ GGUF ë³€í™˜
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ rejected.jsonl     # í‹€ë¦° ë‹µë³€ ëª¨ìŒ
â”‚   â”œâ”€â”€ chosen.jsonl       # í•™ìŠµ í˜ì–´ (Q + rejected + chosen)
â”‚   â””â”€â”€ pending.json       # ìˆ˜ë™ ì‘ì—… í
â””â”€â”€ models/
    â”œâ”€â”€ lora_adapter/      # í•™ìŠµëœ LoRA ì–´ëŒ‘í„°
    â”œâ”€â”€ merged_model/      # ë³‘í•©ëœ HF ëª¨ë¸
    â”œâ”€â”€ model-f16.gguf     # FP16 GGUF
    â””â”€â”€ model-Q4_K_M.gguf  # ì–‘ìí™” GGUF
```

## ğŸ¯ ë°ì´í„° ìˆ˜ì§‘ ì „ëµ

### ìë™ ìˆ˜ì§‘ (ì¼ì¼)

```bash
# cronìœ¼ë¡œ ë§¤ì¼ ì‹¤í–‰
0 3 * * * cd ~/Work/LLM && python dpo/collect-data.py >> dpo/collect.log 2>&1
```

### ìˆ˜ë™ ì‘ì—… ë£¨í‹´

**ë§¤ì£¼ 1íšŒ (30ë¶„):**
1. `python dpo/manual-fix.py`
2. ëŒ€í™”í˜• ëª¨ë“œë¡œ 10~20ê°œ ìˆ˜ì •
3. ì£¼ë§ì— ì§‘ì¤‘ ì‘ì—… (50~100ê°œ)

**ëª©í‘œ:**
- 1ì£¼ì°¨: 50ê°œ
- 2ì£¼ì°¨: 150ê°œ
- 3ì£¼ì°¨: 300ê°œ
- 4ì£¼ì°¨: 500ê°œ â†’ ì²« í•™ìŠµ

### í¬ë¼ìš°ë“œì†Œì‹± (ì„ íƒ)

- ê²Œì„ ì»¤ë®¤ë‹ˆí‹°ì— ìš”ì²­
- JSON í…œí”Œë¦¿ ë°°í¬
- `manual-fix.py` ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°ë¡œ ë³‘í•©

## ğŸ“Š í’ˆì§ˆ ê´€ë¦¬

### í•™ìŠµ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ìµœì†Œ 50ê°œ í˜ì–´ í™•ë³´
- [ ] chosen ë‹µë³€ í’ˆì§ˆ ê²€ìˆ˜
- [ ] ì¤‘ë³µ ì§ˆë¬¸ ì œê±°
- [ ] ê²Œì„ë³„ ê· í˜• (ë§ˆí¬/íŒ°/ì˜¤ë²„ ê° 30%+)

### í•™ìŠµ í›„ ê²€ì¦

```bash
# ìƒˆ ëª¨ë¸ë¡œ QA í…ŒìŠ¤íŠ¸
llmcron restart
sleep 30
python qa-test.py

# ë¡œê·¸ í™•ì¸
tail -100 log/$(date +%Y-%m-%d).md
```

**ê¸°ëŒ€ íš¨ê³¼:**
- ì •í™•ë„ 10~20% í–¥ìƒ
- íŠ¹ì • ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ê°œì„ 
- ë‹µë³€ ì¼ê´€ì„± ì¦ê°€

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. CUDA/MPS ì—†ìŒ

```
âš ï¸  ê²½ê³ : GPU/MPSë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
```

**í•´ê²°:**
- M1/M2/M3/M4 Mac: `pip install torch torchvision`
- NVIDIA GPU: CUDA 11.8+ ì„¤ì¹˜
- CPU í•™ìŠµ: ëŠë¦¬ì§€ë§Œ ê°€ëŠ¥ (1~2ì‹œê°„)

### 2. ë©”ëª¨ë¦¬ ë¶€ì¡±

```
torch.cuda.OutOfMemoryError: ...
```

**í•´ê²°:**
- `train.py` ì—ì„œ `load_in_4bit=True` í™•ì¸
- `per_device_train_batch_size=1` ìœ ì§€
- `gradient_accumulation_steps` ì¦ê°€ (4â†’8)

### 3. llama.cpp ì˜¤ë¥˜

```
âŒ ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤
```

**í•´ê²°:**
```bash
cd ~/Work/LLM
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make
pip install -r requirements.txt
```

## ğŸš€ ê³ ê¸‰ ì˜µì…˜

### A/B í…ŒìŠ¤íŠ¸

```bash
# ê¸°ì¡´ ëª¨ë¸
llama-server -m models/Qwen2.5-7B-Instruct-Q4_K_M.gguf --port 8090

# DPO ëª¨ë¸
llama-server -m models/Qwen2.5-7B-DPO-Q4_K_M.gguf --port 8091

# ë¹„êµ í…ŒìŠ¤íŠ¸
python qa-test.py --port 8090 > baseline.log
python qa-test.py --port 8091 > dpo.log
diff baseline.log dpo.log
```

### ë°˜ë³µ í•™ìŠµ

```bash
# 1ì°¨ í•™ìŠµ (50ê°œ)
python dpo/train.py

# ë°°í¬ + í…ŒìŠ¤íŠ¸
python dpo/convert-to-gguf.py
llmcron restart
python qa-test.py

# ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ (100ê°œ)
python dpo/collect-data.py
python dpo/manual-fix.py

# 2ì°¨ í•™ìŠµ (150ê°œ)
python dpo/train.py
```

### ëª¨ë¸ ë²„ì „ ê´€ë¦¬

```bash
# ë‚ ì§œë³„ ë°±ì—…
cp models/model-Q4_K_M.gguf \
   models/model-Q4_K_M-$(date +%Y%m%d).gguf

# ì„±ëŠ¥ ë¹„êµ ë¡œê·¸
echo "$(date) | v1.0 | ì •í™•ë„ 72% | ë°ì´í„° 150ê°œ" >> models/changelog.txt
```

## ğŸ“ˆ ë¡œë“œë§µ

- [x] ë°ì´í„° ìˆ˜ì§‘ ìë™í™”
- [x] ìˆ˜ë™ ìˆ˜ì • ë„êµ¬
- [x] DPO í•™ìŠµ íŒŒì´í”„ë¼ì¸
- [x] GGUF ë³€í™˜ ìë™í™”
- [ ] ì›¹ UI (ìˆ˜ë™ ì‘ì—…ìš©)
- [ ] ìë™ QA ë¹„êµ (before/after)
- [ ] ë©€í‹° ëª¨ë¸ ì•™ìƒë¸”
- [ ] ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° í¬ë¼ìš°ë“œì†Œì‹±

## ğŸ“š ì°¸ê³ ìë£Œ

- [Unsloth DPO ê°€ì´ë“œ](https://github.com/unslothai/unsloth)
- [TRL DPO Trainer](https://huggingface.co/docs/trl/dpo_trainer)
- [Qwen2.5 ëª¨ë¸](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
- [llama.cpp](https://github.com/ggerganov/llama.cpp)

---

**ë¬¸ì˜:** ì´ë” (@YTRadar)

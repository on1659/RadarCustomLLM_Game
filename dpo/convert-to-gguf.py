#!/usr/bin/env python3
"""LoRA ì–´ëŒ‘í„°ë¥¼ GGUFë¡œ ë³€í™˜
í•™ìŠµí•œ ì–´ëŒ‘í„°ë¥¼ ê¸°ì¡´ ëª¨ë¸ì— ë³‘í•© í›„ GGUF ë³€í™˜
"""

from pathlib import Path
import subprocess

# ê²½ë¡œ
DPO_DIR = Path(__file__).parent
ADAPTER_DIR = DPO_DIR / "models/lora_adapter"
OUTPUT_DIR = DPO_DIR / "models"
LLAMA_CPP_DIR = Path.home() / "Work/LLM/llama.cpp"  # llama.cpp ê²½ë¡œ
BASE_MODEL = "Qwen/Qwen2.5-7B-Instruct"


def check_requirements():
    """í•„ìˆ˜ ì¡°ê±´ í™•ì¸"""
    if not ADAPTER_DIR.exists():
        print("âŒ LoRA ì–´ëŒ‘í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ë¨¼ì € `python dpo/train.py`ë¡œ í•™ìŠµí•˜ì„¸ìš”.")
        return False
    
    if not LLAMA_CPP_DIR.exists():
        print("âŒ llama.cppë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ì˜ˆìƒ ê²½ë¡œ: {LLAMA_CPP_DIR}")
        print(f"   git clone https://github.com/ggerganov/llama.cpp")
        return False
    
    return True


def merge_lora():
    """LoRA ì–´ëŒ‘í„°ë¥¼ ê¸°ë³¸ ëª¨ë¸ì— ë³‘í•©"""
    print("\nğŸ”„ LoRA ì–´ëŒ‘í„° ë³‘í•© ì¤‘...")
    
    try:
        from unsloth import FastLanguageModel
        
        # ëª¨ë¸ + ì–´ëŒ‘í„° ë¡œë“œ
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=str(ADAPTER_DIR),
            max_seq_length=2048,
            dtype=None,
            load_in_4bit=False,  # ë³‘í•© ì‹œì—ëŠ” full precision
        )
        
        # ë³‘í•©
        merged_dir = OUTPUT_DIR / "merged_model"
        merged_dir.mkdir(exist_ok=True)
        
        model = FastLanguageModel.for_inference(model)  # LoRA ë³‘í•©
        model.save_pretrained(str(merged_dir))
        tokenizer.save_pretrained(str(merged_dir))
        
        print(f"âœ… ë³‘í•© ì™„ë£Œ: {merged_dir}")
        return merged_dir
    
    except Exception as e:
        print(f"âŒ ë³‘í•© ì‹¤íŒ¨: {e}")
        return None


def convert_to_gguf(merged_dir):
    """HuggingFace ëª¨ë¸ì„ GGUFë¡œ ë³€í™˜"""
    print("\nğŸ”§ GGUF ë³€í™˜ ì¤‘...")
    
    convert_script = LLAMA_CPP_DIR / "convert_hf_to_gguf.py"
    quantize_bin = LLAMA_CPP_DIR / "llama-quantize"
    
    if not convert_script.exists():
        print(f"âŒ ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {convert_script}")
        return None
    
    # FP16 GGUF ìƒì„±
    fp16_output = OUTPUT_DIR / "model-f16.gguf"
    
    cmd = [
        "python", str(convert_script),
        str(merged_dir),
        "--outfile", str(fp16_output),
        "--outtype", "f16"
    ]
    
    print(f"ì‹¤í–‰: {' '.join(cmd)}")
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode != 0:
        print(f"âŒ ë³€í™˜ ì‹¤íŒ¨:\n{result.stderr}")
        return None
    
    print(f"âœ… FP16 GGUF ìƒì„±: {fp16_output}")
    
    # Q4_K_M ì–‘ìí™”
    if quantize_bin.exists():
        q4_output = OUTPUT_DIR / "model-Q4_K_M.gguf"
        
        cmd = [
            str(quantize_bin),
            str(fp16_output),
            str(q4_output),
            "Q4_K_M"
        ]
        
        print(f"\nğŸ”§ ì–‘ìí™” ì¤‘ (Q4_K_M)...")
        print(f"ì‹¤í–‰: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            print(f"âœ… ì–‘ìí™” ì™„ë£Œ: {q4_output}")
            return q4_output
        else:
            print(f"âš ï¸  ì–‘ìí™” ì‹¤íŒ¨, FP16 ëª¨ë¸ ì‚¬ìš©:\n{result.stderr}")
            return fp16_output
    
    return fp16_output


def deploy(gguf_path):
    """ìƒˆ ëª¨ë¸ì„ ì„œë²„ì— ë°°í¬"""
    print("\nğŸš€ ì„œë²„ ë°°í¬")
    
    target_dir = Path.home() / "Work/LLM/models"
    target_path = target_dir / "Qwen2.5-7B-DPO-Q4_K_M.gguf"
    
    # ë³µì‚¬
    import shutil
    shutil.copy(gguf_path, target_path)
    
    print(f"âœ… ë°°í¬ ì™„ë£Œ: {target_path}")
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print(f"  1. llama-server ì¬ì‹œì‘:")
    print(f"     pkill llama-server")
    print(f"     cd ~/Work/LLM")
    print(f"     nohup ./build/bin/llama-server -m {target_path} -c 8192 --port 8090 > llama-server.log 2>&1 &")
    print(f"\n  2. ë˜ëŠ” ìë™ ì¬ì‹œì‘:")
    print(f"     llmcron restart")


if __name__ == "__main__":
    print("ğŸ”„ LoRA â†’ GGUF ë³€í™˜ íŒŒì´í”„ë¼ì¸\n")
    
    if not check_requirements():
        exit(1)
    
    # 1. LoRA ë³‘í•©
    merged_dir = merge_lora()
    if not merged_dir:
        exit(1)
    
    # 2. GGUF ë³€í™˜
    gguf_path = convert_to_gguf(merged_dir)
    if not gguf_path:
        exit(1)
    
    # 3. ë°°í¬
    deploy(gguf_path)
    
    print("\nâœ… ëª¨ë“  ê³¼ì • ì™„ë£Œ!")

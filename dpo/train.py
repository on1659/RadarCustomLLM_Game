#!/usr/bin/env python3
"""DPO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (TRL ì§ì ‘ ì‚¬ìš©, unsloth ë¶ˆí•„ìš”)
chosen/rejected í˜ì–´ë¡œ ëª¨ë¸ í•™ìŠµ
"""

import json
from pathlib import Path
from datasets import Dataset
import torch
import sys

# venv í™œì„±í™” í™•ì¸
if not hasattr(sys, 'real_prefix') and not (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
    print("âš ï¸  venvê°€ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   source ~/Work/LLM/dpo/venv/bin/activate")
    exit(1)

# ì„¤ì •
DATASET_DIR = Path(__file__).parent / "dataset"
CHOSEN_FILE = DATASET_DIR / "chosen.jsonl"
OUTPUT_DIR = Path(__file__).parent / "models"
OUTPUT_DIR.mkdir(exist_ok=True)

# ëª¨ë¸ ê²½ë¡œ
BASE_MODEL = "Qwen/Qwen2.5-3B-Instruct"


def load_dataset():
    """chosen.jsonl ë¡œë“œ"""
    data = []
    with open(CHOSEN_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    
    print(f"ğŸ“š ë°ì´í„°ì…‹ ë¡œë“œ: {len(data)}ê°œ í˜ì–´")
    
    if len(data) < 10:
        print("âš ï¸  ê²½ê³ : ë°ì´í„°ê°€ 10ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. ìµœì†Œ 50ê°œ ì´ìƒ ê¶Œì¥!")
        choice = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if choice != 'y':
            return None
    
    return data


def prepare_dpo_dataset(data):
    """DPO í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    formatted = []
    
    for item in data:
        formatted.append({
            "prompt": item['question'],
            "chosen": item["chosen"],
            "rejected": item["rejected"]
        })
    
    return Dataset.from_list(formatted)


def train_dpo():
    """DPO í•™ìŠµ ì‹¤í–‰"""
    print("\nğŸš€ DPO í•™ìŠµ ì‹œì‘ (TRL)\n")
    
    # ë°ì´í„° ë¡œë“œ
    data = load_dataset()
    if not data:
        return
    
    dataset = prepare_dpo_dataset(data)
    print(f"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: {len(dataset)}ê°œ")
    
    try:
        print("\nğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì¤‘...")
        from transformers import (
            AutoModelForCausalLM,
            AutoTokenizer,
            BitsAndBytesConfig,
            TrainingArguments
        )
        from peft import LoraConfig, get_peft_model
        from trl import DPOTrainer, DPOConfig
        
        # ëª¨ë¸ ë¡œë“œ (4bit)
        print(f"\nğŸ”§ ëª¨ë¸ ë¡œë“œ: {BASE_MODEL}")
        
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            llm_int8_enable_fp32_cpu_offload=True,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
        )
        
        model_ref = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
        )
        
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # LoRA ì„¤ì •
        print("\nğŸ¯ LoRA ì–´ëŒ‘í„° ì„¤ì •")
        peft_config = LoraConfig(
            r=16,
            lora_alpha=32,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", 
                          "gate_proj", "up_proj", "down_proj"],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )
        
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()
        
        # DPO í•™ìŠµ ì„¤ì •
        print("\nâš™ï¸  í•™ìŠµ ì„¤ì •")
        training_args = DPOConfig(
            output_dir=str(OUTPUT_DIR / "checkpoints"),
            per_device_train_batch_size=1,
            gradient_accumulation_steps=4,
            num_train_epochs=3,
            learning_rate=5e-5,
            fp16=True,
            logging_steps=5,
            save_steps=100,
            optim="adamw_8bit",
            warmup_ratio=0.1,
            max_length=2048,
            max_prompt_length=1024,
            beta=0.1,
            remove_unused_columns=False,
        )
        
        # Tokenize í•¨ìˆ˜
        def tokenize_fn(example):
            prompt_full = f"<|im_start|>user\n{example['prompt']}<|im_end|>\n<|im_start|>assistant\n"
            
            prompt_tokens = tokenizer(prompt_full, truncation=True, max_length=1024)
            chosen_tokens = tokenizer(example['chosen'], truncation=True, max_length=1024)
            rejected_tokens = tokenizer(example['rejected'], truncation=True, max_length=1024)
            
            return {
                "prompt": prompt_full,
                "chosen": example['chosen'],
                "rejected": example['rejected'],
            }
        
        dataset = dataset.map(tokenize_fn)
        
        trainer = DPOTrainer(
            model=model,
            ref_model=model_ref,
            args=training_args,
            train_dataset=dataset,
            tokenizer=tokenizer,
        )
        
        # í•™ìŠµ ì‹œì‘
        print("\nğŸ”¥ í•™ìŠµ ì‹œì‘!")
        print(f"  - ë°ì´í„°: {len(dataset)}ê°œ")
        print(f"  - ì—í­: {training_args.num_train_epochs}")
        print(f"  - ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size}")
        print(f"  - ì˜ˆìƒ ì‹œê°„: ~{len(dataset) * training_args.num_train_epochs // 10} ë¶„\n")
        
        trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        model.save_pretrained(str(OUTPUT_DIR / "lora_adapter"))
        tokenizer.save_pretrained(str(OUTPUT_DIR / "lora_adapter"))
        
        print("\nâœ… í•™ìŠµ ì™„ë£Œ!")
        print(f"\nì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR / 'lora_adapter'}")
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("  1. python3 dpo/convert-to-gguf.py  # GGUF ë³€í™˜")
        print("  2. llmcron restart                 # ì„œë²„ ì¬ì‹œì‘")
    
    except ImportError as e:
        print(f"\nâŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”:")
        print(f"   source ~/Work/LLM/dpo/venv/bin/activate")
        print(f"   pip install transformers trl peft bitsandbytes")
        print(f"\nìƒì„¸ ì˜¤ë¥˜: {e}")
    
    except Exception as e:
        print(f"\nâŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # CUDA/MPS ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    if not torch.cuda.is_available() and not torch.backends.mps.is_available():
        print("âš ï¸  ê²½ê³ : GPU/MPSë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   CPUë¡œ í•™ìŠµí•˜ë©´ ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        choice = input("\nê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if choice != 'y':
            exit(0)
    
    train_dpo()
